{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation\n",
    "\n",
    "!pip install asf_search sentinelhub rasterio geopandas shapely numpy scipy scikit-image matplotlib folium earthaccess --quiet\n",
    "!pip install rioxarray xarray netCDF4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n",
      "ASF Search version: 10.1.2\n"
     ]
    }
   ],
   "source": [
    "#Importing all necessary libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Geospatial libraries\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.merge import merge\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box, Polygon\n",
    "import folium\n",
    "\n",
    "# Data access\n",
    "import asf_search as asf\n",
    "import earthaccess\n",
    "\n",
    "# Image processing\n",
    "from scipy.ndimage import uniform_filter, median_filter\n",
    "from skimage import exposure\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(f\"ASF Search version: {asf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created: ./data\n",
      "✓ Created: ./data/roi\n",
      "✓ Created: ./data/sentinel1\n",
      "✓ Created: ./data/sentinel2\n",
      "✓ Created: ./output\n",
      "✓ Created: ./output/maps\n",
      "✓ Created: ./output/changes\n",
      "✓ Created: ./reports\n",
      "\n",
      "============================================================\n",
      "AUTHENTICATION SETUP\n",
      "============================================================\n",
      "\n",
      "✓ Authentication complete!\n",
      "✓ Ready to download data from ASF and NASA Earthdata\n"
     ]
    }
   ],
   "source": [
    "#Creating directory structure\n",
    "\n",
    "directories = {\n",
    "    'data': './data',\n",
    "    'data_roi': './data/roi',\n",
    "    'data_s1': './data/sentinel1',\n",
    "    'data_s2': './data/sentinel2',\n",
    "    'output': './output',\n",
    "    'output_maps': './output/maps',\n",
    "    'output_changes': './output/changes',\n",
    "    'reports': './reports'\n",
    "}\n",
    "\n",
    "for name, path in directories.items():\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✓ Created: {path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AUTHENTICATION SETUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Earthdata Authentication\n",
    "# You'll be prompted to enter credentials\n",
    "earthaccess.login(strategy=\"interactive\")\n",
    "\n",
    "print(\"\\n✓ Authentication complete!\")\n",
    "print(\"✓ Ready to download data from ASF and NASA Earthdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved Hirakud_Reservoir.geojson\n",
      "✓ Saved Chilika_Lake.geojson\n",
      "\n",
      "✓ ROI map saved to: ./output/maps/roi_locations.html\n",
      "✓ Open this file in a browser to view the locations\n",
      "\n",
      "============================================================\n",
      "ROI SUMMARY\n",
      "============================================================\n",
      "\n",
      "Hirakud_Reservoir:\n",
      "  Bounding Box: [83.85, 21.45, 84.05, 21.6]\n",
      "  Center: [83.95, 21.525]\n",
      "  Hirakud Dam reservoir with seasonal water level variations\n",
      "\n",
      "Chilika_Lake:\n",
      "  Bounding Box: [85.2, 19.65, 85.5, 19.8]\n",
      "  Center: [85.35, 19.725]\n",
      "  Chilika coastal lagoon with tidal and seasonal variations\n",
      "\n",
      "Pre-Monsoon (Apr-May 2024): 2024-04-01 to 2024-05-31\n",
      "Post-Monsoon (Sep-Oct 2024): 2024-09-01 to 2024-10-31\n"
     ]
    }
   ],
   "source": [
    "# ROI 1: Hirakud Reservoir, Odisha - One of India's longest dams\n",
    "roi1_hirakud = {\n",
    "    'name': 'Hirakud_Reservoir',\n",
    "    'bbox': [83.85, 21.45, 84.05, 21.60],  # [min_lon, min_lat, max_lon, max_lat]\n",
    "    'center': [83.95, 21.525],\n",
    "    'description': 'Hirakud Dam reservoir with seasonal water level variations'\n",
    "}\n",
    "\n",
    "# ROI 2: Chilika Lake, Odisha - Largest coastal lagoon in India\n",
    "roi2_chilika = {\n",
    "    'name': 'Chilika_Lake',\n",
    "    'bbox': [85.20, 19.65, 85.50, 19.80],\n",
    "    'center': [85.35, 19.725],\n",
    "    'description': 'Chilika coastal lagoon with tidal and seasonal variations'\n",
    "}\n",
    "\n",
    "rois = [roi1_hirakud, roi2_chilika]\n",
    "\n",
    "# Define time periods\n",
    "pre_monsoon = {\n",
    "    'start': '2024-04-01',\n",
    "    'end': '2024-05-31',\n",
    "    'label': 'Pre-Monsoon (Apr-May 2024)'\n",
    "}\n",
    "\n",
    "post_monsoon = {\n",
    "    'start': '2024-09-01',\n",
    "    'end': '2024-10-31',\n",
    "    'label': 'Post-Monsoon (Sep-Oct 2024)'\n",
    "}\n",
    "\n",
    "# Visualize ROIs\n",
    "m = folium.Map(location=[20.5, 84.5], zoom_start=7)\n",
    "\n",
    "for roi in rois:\n",
    "    # Add rectangle for ROI\n",
    "    folium.Rectangle(\n",
    "        bounds=[[roi['bbox'][1], roi['bbox'][0]], [roi['bbox'][3], roi['bbox'][2]]],\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fillOpacity=0.2,\n",
    "        popup=f\"{roi['name']}<br>{roi['description']}\"\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add marker\n",
    "    folium.Marker(\n",
    "        location=roi['center'],\n",
    "        popup=roi['name'],\n",
    "        icon=folium.Icon(color='red', icon='info-sign')\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save ROIs as GeoJSON\n",
    "for roi in rois:\n",
    "    bbox = roi['bbox']\n",
    "    geom = box(bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "\n",
    "    # FIX: convert list fields to strings to avoid GeoJSON error\n",
    "    roi_clean = roi.copy()\n",
    "    roi_clean['bbox'] = str(roi_clean['bbox'])\n",
    "    roi_clean['center'] = str(roi_clean['center'])\n",
    "\n",
    "    gdf = gpd.GeoDataFrame([roi_clean], geometry=[geom], crs='EPSG:4326')\n",
    "    gdf.to_file(f\"./data/roi/{roi['name']}.geojson\", driver='GeoJSON')\n",
    "    print(f\"✓ Saved {roi['name']}.geojson\")\n",
    "\n",
    "# Display map\n",
    "m.save('./output/maps/roi_locations.html')\n",
    "print(\"\\n✓ ROI map saved to: ./output/maps/roi_locations.html\")\n",
    "print(\"✓ Open this file in a browser to view the locations\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ROI SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for roi in rois:\n",
    "    print(f\"\\n{roi['name']}:\")\n",
    "    print(f\"  Bounding Box: {roi['bbox']}\")\n",
    "    print(f\"  Center: {roi['center']}\")\n",
    "    print(f\"  {roi['description']}\")\n",
    "\n",
    "print(f\"\\n{pre_monsoon['label']}: {pre_monsoon['start']} to {pre_monsoon['end']}\")\n",
    "print(f\"{post_monsoon['label']}: {post_monsoon['start']} to {post_monsoon['end']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SENTINEL-1 DATA SEARCH AND DOWNLOAD\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ROI: Hirakud_Reservoir - Pre-Monsoon\n",
      "============================================================\n",
      "\n",
      "Searching Hirakud_Reservoir: 2024-04-01 to 2024-05-31\n",
      "  Found 10 scenes\n",
      "\n",
      "Selected scene for Pre-Monsoon:\n",
      "  Scene: S1A_IW_GRDH_1SDV_20240523T002148_20240523T002213_053991_069045_4E3F\n",
      "  Date: 2024-05-23T00:21:48Z\n",
      "  Polarization: VV+VH\n",
      "\n",
      "============================================================\n",
      "ROI: Hirakud_Reservoir - Post-Monsoon\n",
      "============================================================\n",
      "\n",
      "Searching Hirakud_Reservoir: 2024-09-01 to 2024-10-31\n",
      "  Found 8 scenes\n",
      "\n",
      "Selected scene for Post-Monsoon:\n",
      "  Scene: S1A_IW_GRDH_1SDV_20241026T002147_20241026T002212_056266_06E3CF_B7C4\n",
      "  Date: 2024-10-26T00:21:47Z\n",
      "  Polarization: VV+VH\n",
      "\n",
      "============================================================\n",
      "ROI: Chilika_Lake - Pre-Monsoon\n",
      "============================================================\n",
      "\n",
      "Searching Chilika_Lake: 2024-04-01 to 2024-05-31\n",
      "  Found 10 scenes\n",
      "\n",
      "Selected scene for Pre-Monsoon:\n",
      "  Scene: S1A_IW_GRDH_1SDV_20240530T001406_20240530T001431_054093_0693D5_70AF\n",
      "  Date: 2024-05-30T00:14:06Z\n",
      "  Polarization: VV+VH\n",
      "\n",
      "============================================================\n",
      "ROI: Chilika_Lake - Post-Monsoon\n",
      "============================================================\n",
      "\n",
      "Searching Chilika_Lake: 2024-09-01 to 2024-10-31\n",
      "  Found 10 scenes\n",
      "\n",
      "Selected scene for Post-Monsoon:\n",
      "  Scene: S1A_IW_GRDH_1SDV_20241021T001405_20241021T001430_056193_06E0E9_5A1D\n",
      "  Date: 2024-10-21T00:14:05Z\n",
      "  Polarization: VV+VH\n",
      "\n",
      "============================================================\n",
      "SENTINEL-1 SEARCH COMPLETE\n",
      "============================================================\n",
      "Total scenes identified: 4\n"
     ]
    }
   ],
   "source": [
    "#Search Sentinel-1 GRD data from ASF\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SENTINEL-1 DATA SEARCH AND DOWNLOAD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def search_sentinel1_asf(roi, start_date, end_date, max_results=5):\n",
    "  \n",
    "    bbox = roi['bbox']\n",
    "    \n",
    "    \n",
    "    wkt = f\"POLYGON(({bbox[0]} {bbox[1]}, {bbox[2]} {bbox[1]}, {bbox[2]} {bbox[3]}, {bbox[0]} {bbox[3]}, {bbox[0]} {bbox[1]}))\"\n",
    "    \n",
    "    print(f\"\\nSearching {roi['name']}: {start_date} to {end_date}\")\n",
    "    \n",
    "    results = asf.geo_search(\n",
    "        platform=[asf.PLATFORM.SENTINEL1],\n",
    "        processingLevel=asf.PRODUCT_TYPE.GRD_HD,  # High resolution GRD\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        intersectsWith=wkt,\n",
    "        maxResults=max_results\n",
    "    )\n",
    "    \n",
    "    print(f\"  Found {len(results)} scenes\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def download_sentinel1_scene(scene, roi_name, period_label, download_dir):\n",
    "   \n",
    "    session = asf.ASFSession().auth_with_creds(os.environ.get('EARTHDATA_USERNAME'), \n",
    "                                                 os.environ.get('EARTHDATA_PASSWORD'))\n",
    "    \n",
    "    \n",
    "    scene_dir = f\"{download_dir}/{roi_name}_{period_label}\"\n",
    "    Path(scene_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"  Downloading: {scene.properties['sceneName']}\")\n",
    "    print(f\"  Acquisition: {scene.properties['startTime']}\")\n",
    "    print(f\"  Orbit: {scene.properties['flightDirection']}\")\n",
    "    \n",
    "  \n",
    "    scene.download(path=scene_dir, session=session)\n",
    "    \n",
    "    return scene_dir\n",
    "\n",
    "\n",
    "s1_scenes = {}\n",
    "\n",
    "\n",
    "for roi in rois:\n",
    "    roi_name = roi['name']\n",
    "    s1_scenes[roi_name] = {}\n",
    "    \n",
    "    # Pre-monsoon\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ROI: {roi_name} - Pre-Monsoon\")\n",
    "    print(f\"{'='*60}\")\n",
    "    results_pre = search_sentinel1_asf(roi, pre_monsoon['start'], pre_monsoon['end'], max_results=10)\n",
    "    \n",
    "    if len(results_pre) > 0:\n",
    "        # Filter for best scene (lowest cloud cover, recent date)\n",
    "        # For GRD, select the most recent scene\n",
    "        scene_pre = results_pre[0]  # Most recent\n",
    "        s1_scenes[roi_name]['pre_monsoon'] = scene_pre\n",
    "        \n",
    "        print(f\"\\nSelected scene for Pre-Monsoon:\")\n",
    "        print(f\"  Scene: {scene_pre.properties['sceneName']}\")\n",
    "        print(f\"  Date: {scene_pre.properties['startTime']}\")\n",
    "        print(f\"  Polarization: {scene_pre.properties['polarization']}\")\n",
    "    else:\n",
    "        print(\"  WARNING: No scenes found for pre-monsoon period\")\n",
    "    \n",
    "    # Post-monsoon\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ROI: {roi_name} - Post-Monsoon\")\n",
    "    print(f\"{'='*60}\")\n",
    "    results_post = search_sentinel1_asf(roi, post_monsoon['start'], post_monsoon['end'], max_results=10)\n",
    "    \n",
    "    if len(results_post) > 0:\n",
    "        scene_post = results_post[0]\n",
    "        s1_scenes[roi_name]['post_monsoon'] = scene_post\n",
    "        \n",
    "        print(f\"\\nSelected scene for Post-Monsoon:\")\n",
    "        print(f\"  Scene: {scene_post.properties['sceneName']}\")\n",
    "        print(f\"  Date: {scene_post.properties['startTime']}\")\n",
    "        print(f\"  Polarization: {scene_post.properties['polarization']}\")\n",
    "    else:\n",
    "        print(\"  WARNING: No scenes found for post-monsoon period\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENTINEL-1 SEARCH COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total scenes identified: {sum(len(v) for v in s1_scenes.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DOWNLOADING SENTINEL-1 SCENES\n",
      "============================================================\n",
      "Note: Each scene is approximately 1GB. Download may take 5-10 minutes per scene.\n",
      "============================================================\n",
      "\n",
      "✓ Authentication successful!\n",
      "\n",
      "============================================================\n",
      "Downloading: Hirakud_Reservoir - pre_monsoon\n",
      "============================================================\n",
      "  Scene: S1A_IW_GRDH_1SDV_20240523T002148_20240523T002213_053991_069045_4E3F\n",
      "  Acquisition: 2024-05-23T00:21:48Z\n",
      "  Polarization: VV+VH\n",
      "  Orbit: DESCENDING\n",
      "\n",
      "  Starting download...\n",
      "  ✓ Download complete: ./data/sentinel1/Hirakud_Reservoir_pre_monsoon\n",
      "\n",
      "============================================================\n",
      "Downloading: Hirakud_Reservoir - post_monsoon\n",
      "============================================================\n",
      "  Scene: S1A_IW_GRDH_1SDV_20241026T002147_20241026T002212_056266_06E3CF_B7C4\n",
      "  Acquisition: 2024-10-26T00:21:47Z\n",
      "  Polarization: VV+VH\n",
      "  Orbit: DESCENDING\n",
      "\n",
      "  Starting download...\n",
      "  ✓ Download complete: ./data/sentinel1/Hirakud_Reservoir_post_monsoon\n",
      "\n",
      "============================================================\n",
      "Downloading: Chilika_Lake - pre_monsoon\n",
      "============================================================\n",
      "  Scene: S1A_IW_GRDH_1SDV_20240530T001406_20240530T001431_054093_0693D5_70AF\n",
      "  Acquisition: 2024-05-30T00:14:06Z\n",
      "  Polarization: VV+VH\n",
      "  Orbit: DESCENDING\n",
      "\n",
      "  Starting download...\n",
      "  ✓ Download complete: ./data/sentinel1/Chilika_Lake_pre_monsoon\n",
      "\n",
      "============================================================\n",
      "Downloading: Chilika_Lake - post_monsoon\n",
      "============================================================\n",
      "  Scene: S1A_IW_GRDH_1SDV_20241021T001405_20241021T001430_056193_06E0E9_5A1D\n",
      "  Acquisition: 2024-10-21T00:14:05Z\n",
      "  Polarization: VV+VH\n",
      "  Orbit: DESCENDING\n",
      "\n",
      "  Starting download...\n",
      "  ✓ Download complete: ./data/sentinel1/Chilika_Lake_post_monsoon\n",
      "\n",
      "============================================================\n",
      "SENTINEL-1 DOWNLOAD SUMMARY\n",
      "============================================================\n",
      "\n",
      "Hirakud_Reservoir:\n",
      "  pre_monsoon: ✓ Success\n",
      "    Path: ./data/sentinel1/Hirakud_Reservoir_pre_monsoon\n",
      "    Scene: S1A_IW_GRDH_1SDV_20240523T002148_20240523T002213_053991_069045_4E3F\n",
      "  post_monsoon: ✓ Success\n",
      "    Path: ./data/sentinel1/Hirakud_Reservoir_post_monsoon\n",
      "    Scene: S1A_IW_GRDH_1SDV_20241026T002147_20241026T002212_056266_06E3CF_B7C4\n",
      "\n",
      "Chilika_Lake:\n",
      "  pre_monsoon: ✓ Success\n",
      "    Path: ./data/sentinel1/Chilika_Lake_pre_monsoon\n",
      "    Scene: S1A_IW_GRDH_1SDV_20240530T001406_20240530T001431_054093_0693D5_70AF\n",
      "  post_monsoon: ✓ Success\n",
      "    Path: ./data/sentinel1/Chilika_Lake_post_monsoon\n",
      "    Scene: S1A_IW_GRDH_1SDV_20241021T001405_20241021T001430_056193_06E0E9_5A1D\n",
      "\n",
      "✓ Download log saved to: ./data/sentinel1/download_log.json\n"
     ]
    }
   ],
   "source": [
    "#Download Sentinel-1 Data\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOWNLOADING SENTINEL-1 SCENES\")\n",
    "print(\"=\"*60)\n",
    "print(\"Note: Each scene is approximately 1GB. Download may take 5-10 minutes per scene.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "import getpass\n",
    "\n",
    "if 'EARTHDATA_USERNAME' not in os.environ or 'EARTHDATA_PASSWORD' not in os.environ:\n",
    "    print(\"\\nPlease enter your NASA Earthdata credentials:\")\n",
    "    username = input(\"Username: \")\n",
    "    password = getpass.getpass(\"Password: \")\n",
    "    os.environ['EARTHDATA_USERNAME'] = username\n",
    "    os.environ['EARTHDATA_PASSWORD'] = password\n",
    "else:\n",
    "    username = os.environ['EARTHDATA_USERNAME']\n",
    "    password = os.environ['EARTHDATA_PASSWORD']\n",
    "\n",
    "\n",
    "session = asf.ASFSession().auth_with_creds(username, password)\n",
    "\n",
    "print(\"\\n✓ Authentication successful!\")\n",
    "\n",
    "downloaded_s1 = {}\n",
    "\n",
    "for roi_name, periods in s1_scenes.items():\n",
    "    downloaded_s1[roi_name] = {}\n",
    "    \n",
    "    for period_name, scene in periods.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Downloading: {roi_name} - {period_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Create subdirectory\n",
    "            scene_dir = f\"./data/sentinel1/{roi_name}_{period_name}\"\n",
    "            Path(scene_dir).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            print(f\"  Scene: {scene.properties['sceneName']}\")\n",
    "            print(f\"  Acquisition: {scene.properties['startTime']}\")\n",
    "            print(f\"  Polarization: {scene.properties['polarization']}\")\n",
    "            print(f\"  Orbit: {scene.properties['flightDirection']}\")\n",
    "            print(f\"\\n  Starting download...\")\n",
    "            \n",
    "            \n",
    "            scene.download(path=scene_dir, session=session)\n",
    "            \n",
    "            downloaded_s1[roi_name][period_name] = {\n",
    "                'path': scene_dir,\n",
    "                'scene_name': scene.properties['sceneName'],\n",
    "                'date': scene.properties['startTime']\n",
    "            }\n",
    "            print(f\"  ✓ Download complete: {scene_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Download failed: {e}\")\n",
    "            downloaded_s1[roi_name][period_name] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENTINEL-1 DOWNLOAD SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for roi_name, periods in downloaded_s1.items():\n",
    "    print(f\"\\n{roi_name}:\")\n",
    "    for period, data in periods.items():\n",
    "        if data:\n",
    "            print(f\"  {period}: ✓ Success\")\n",
    "            print(f\"    Path: {data['path']}\")\n",
    "            print(f\"    Scene: {data['scene_name']}\")\n",
    "        else:\n",
    "            print(f\"  {period}: ✗ Failed\")\n",
    "\n",
    "\n",
    "with open('./data/sentinel1/download_log.json', 'w') as f:\n",
    "    log = {\n",
    "        'download_time': datetime.now().isoformat(),\n",
    "        'scenes': downloaded_s1\n",
    "    }\n",
    "    json.dump(log, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n✓ Download log saved to: ./data/sentinel1/download_log.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SENTINEL-2 DATA SEARCH AND DOWNLOAD\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ROI: Hirakud_Reservoir - Pre-Monsoon\n",
      "============================================================\n",
      "\n",
      "Searching Hirakud_Reservoir: 2024-04-01 to 2024-05-31\n",
      "  Max cloud cover: 30%\n",
      "  Found 20 scenes with <30% cloud cover\n",
      "\n",
      "Selected scene for Pre-Monsoon:\n",
      "  Scene ID: S2A_44QQJ_20240514_0_L2A\n",
      "  Date: 2024-05-14\n",
      "  Cloud cover: 0.022878%\n",
      "\n",
      "============================================================\n",
      "ROI: Hirakud_Reservoir - Post-Monsoon\n",
      "============================================================\n",
      "\n",
      "Searching Hirakud_Reservoir: 2024-09-01 to 2024-10-31\n",
      "  Max cloud cover: 30%\n",
      "  Found 18 scenes with <30% cloud cover\n",
      "\n",
      "Selected scene for Post-Monsoon:\n",
      "  Scene ID: S2B_44QQK_20241006_0_L2A\n",
      "  Date: 2024-10-06\n",
      "  Cloud cover: 10.98659%\n",
      "\n",
      "============================================================\n",
      "ROI: Chilika_Lake - Pre-Monsoon\n",
      "============================================================\n",
      "\n",
      "Searching Chilika_Lake: 2024-04-01 to 2024-05-31\n",
      "  Max cloud cover: 30%\n",
      "  Found 15 scenes with <30% cloud cover\n",
      "\n",
      "Selected scene for Pre-Monsoon:\n",
      "  Scene ID: S2B_45QUC_20240416_0_L2A\n",
      "  Date: 2024-04-16\n",
      "  Cloud cover: 0.03637%\n",
      "\n",
      "============================================================\n",
      "ROI: Chilika_Lake - Post-Monsoon\n",
      "============================================================\n",
      "\n",
      "Searching Chilika_Lake: 2024-09-01 to 2024-10-31\n",
      "  Max cloud cover: 30%\n",
      "  Found 7 scenes with <30% cloud cover\n",
      "\n",
      "Selected scene for Post-Monsoon:\n",
      "  Scene ID: S2A_45QUB_20240928_0_L2A\n",
      "  Date: 2024-09-28\n",
      "  Cloud cover: 9.932241%\n",
      "\n",
      "============================================================\n",
      "SENTINEL-2 SEARCH COMPLETE\n",
      "============================================================\n",
      "Total scenes identified: 4\n"
     ]
    }
   ],
   "source": [
    "#Search Sentinel-2 L2A from AWS Open Data\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SENTINEL-2 DATA SEARCH AND DOWNLOAD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Install additional library if needed\n",
    "try:\n",
    "    import pystac_client\n",
    "    from pystac_client import Client\n",
    "except:\n",
    "    !pip install pystac-client --quiet\n",
    "    import pystac_client\n",
    "    from pystac_client import Client\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def search_sentinel2_aws(roi, start_date, end_date, max_cloud_cover=30):\n",
    "   \n",
    "    bbox = roi['bbox']\n",
    "    \n",
    "    print(f\"\\nSearching {roi['name']}: {start_date} to {end_date}\")\n",
    "    print(f\"  Max cloud cover: {max_cloud_cover}%\")\n",
    "    \n",
    "\n",
    "    catalog = Client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "    \n",
    "\n",
    "    search = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        bbox=bbox,\n",
    "        datetime=f\"{start_date}/{end_date}\",\n",
    "        query={\"eo:cloud_cover\": {\"lt\": max_cloud_cover}},\n",
    "        max_items=20\n",
    "    )\n",
    "    \n",
    "    items = list(search.items())\n",
    "    print(f\"  Found {len(items)} scenes with <{max_cloud_cover}% cloud cover\")\n",
    "    \n",
    " \n",
    "    items_sorted = sorted(items, key=lambda x: x.properties.get('eo:cloud_cover', 100))\n",
    "    \n",
    "    return items_sorted\n",
    "\n",
    "def download_sentinel2_bands(item, roi, bands=['B02', 'B03', 'B04', 'B08', 'B11', 'SCL'], \n",
    "                             output_dir='./data/sentinel2'):\n",
    "\n",
    "    scene_id = item.id\n",
    "    date = item.properties['datetime'][:10]\n",
    "    \n",
    "    print(f\"\\n  Downloading scene: {scene_id}\")\n",
    "    print(f\"  Date: {date}\")\n",
    "    print(f\"  Cloud cover: {item.properties.get('eo:cloud_cover', 'N/A')}%\")\n",
    "    \n",
    "    # Create scene directory\n",
    "    scene_dir = f\"{output_dir}/{roi['name']}_{date}\"\n",
    "    Path(scene_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    downloaded_files = {}\n",
    "    \n",
    "    for band in bands:\n",
    "        try:\n",
    "            if band in item.assets:\n",
    "                asset = item.assets[band]\n",
    "                url = asset.href\n",
    "                \n",
    "                # Download file\n",
    "                filename = f\"{scene_dir}/{band}.tif\"\n",
    "                \n",
    "                if not os.path.exists(filename):\n",
    "                    print(f\"    Downloading {band}...\", end='')\n",
    "                    response = requests.get(url, stream=True)\n",
    "                    response.raise_for_status()\n",
    "                    \n",
    "                    with open(filename, 'wb') as f:\n",
    "                        for chunk in response.iter_content(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "                    print(\" ✓\")\n",
    "                else:\n",
    "                    print(f\"    {band} already exists ✓\")\n",
    "                \n",
    "                downloaded_files[band] = filename\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Failed to download {band}: {e}\")\n",
    "    \n",
    "    return scene_dir, downloaded_files\n",
    "\n",
    "\n",
    "s2_items = {}\n",
    "\n",
    "\n",
    "for roi in rois:\n",
    "    roi_name = roi['name']\n",
    "    s2_items[roi_name] = {}\n",
    "    \n",
    "   \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ROI: {roi_name} - Pre-Monsoon\")\n",
    "    print(f\"{'='*60}\")\n",
    "    results_pre = search_sentinel2_aws(roi, pre_monsoon['start'], pre_monsoon['end'], max_cloud_cover=30)\n",
    "    \n",
    "    if len(results_pre) > 0:\n",
    "        # Select scene with lowest cloud cover\n",
    "        scene_pre = results_pre[0]\n",
    "        s2_items[roi_name]['pre_monsoon'] = scene_pre\n",
    "        \n",
    "        print(f\"\\nSelected scene for Pre-Monsoon:\")\n",
    "        print(f\"  Scene ID: {scene_pre.id}\")\n",
    "        print(f\"  Date: {scene_pre.properties['datetime'][:10]}\")\n",
    "        print(f\"  Cloud cover: {scene_pre.properties.get('eo:cloud_cover', 'N/A')}%\")\n",
    "    else:\n",
    "        print(\"  WARNING: No suitable scenes found for pre-monsoon period\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ROI: {roi_name} - Post-Monsoon\")\n",
    "    print(f\"{'='*60}\")\n",
    "    results_post = search_sentinel2_aws(roi, post_monsoon['start'], post_monsoon['end'], max_cloud_cover=30)\n",
    "    \n",
    "    if len(results_post) > 0:\n",
    "        scene_post = results_post[0]\n",
    "        s2_items[roi_name]['post_monsoon'] = scene_post\n",
    "        \n",
    "        print(f\"\\nSelected scene for Post-Monsoon:\")\n",
    "        print(f\"  Scene ID: {scene_post.id}\")\n",
    "        print(f\"  Date: {scene_post.properties['datetime'][:10]}\")\n",
    "        print(f\"  Cloud cover: {scene_post.properties.get('eo:cloud_cover', 'N/A')}%\")\n",
    "    else:\n",
    "        print(\"  WARNING: No suitable scenes found for post-monsoon period\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENTINEL-2 SEARCH COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total scenes identified: {sum(len(v) for v in s2_items.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING AVAILABLE SENTINEL-2 ASSETS\n",
      "============================================================\n",
      "\n",
      "Hirakud_Reservoir - pre_monsoon:\n",
      "  Scene: S2A_44QQJ_20240514_0_L2A\n",
      "  Available assets: ['aot', 'blue', 'coastal', 'granule_metadata', 'green', 'nir', 'nir08', 'nir09', 'red', 'rededge1']...\n",
      "\n",
      "============================================================\n",
      "DOWNLOADING SENTINEL-2 BANDS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Downloading: Hirakud_Reservoir - pre_monsoon\n",
      "============================================================\n",
      "\n",
      "  Scene: S2A_44QQJ_20240514_0_L2A\n",
      "  Date: 2024-05-14\n",
      "  Cloud cover: 0.022878%\n",
      " ✓ (214.9 MB)ng B02 (as blue)...\n",
      " ✓ (217.5 MB)ng B03 (as green)...\n",
      " ✓ (227.8 MB)ng B04 (as red)...\n",
      " ✓ (232.2 MB)ng B08 (as nir)...\n",
      " ✓ (58.2 MB)ing B11 (as swir16)...\n",
      " ✓ (3.7 MB)ding SCL (as scl)...\n",
      "\n",
      "  ✓ Successfully downloaded 6 bands to: ./data/sentinel2/Hirakud_Reservoir_2024-05-14\n",
      "\n",
      "============================================================\n",
      "Downloading: Hirakud_Reservoir - post_monsoon\n",
      "============================================================\n",
      "\n",
      "  Scene: S2B_44QQK_20241006_0_L2A\n",
      "  Date: 2024-10-06\n",
      "  Cloud cover: 10.98659%\n",
      " ✓ (198.0 MB)ng B02 (as blue)...\n",
      " ✓ (206.6 MB)ng B03 (as green)...\n",
      " ✓ (206.2 MB)ng B04 (as red)...\n",
      " ✓ (230.5 MB)ng B08 (as nir)...\n",
      " ✓ (56.7 MB)ing B11 (as swir16)...\n",
      " ✓ (2.3 MB)ding SCL (as scl)...\n",
      "\n",
      "  ✓ Successfully downloaded 6 bands to: ./data/sentinel2/Hirakud_Reservoir_2024-10-06\n",
      "\n",
      "============================================================\n",
      "Downloading: Chilika_Lake - pre_monsoon\n",
      "============================================================\n",
      "\n",
      "  Scene: S2B_45QUC_20240416_0_L2A\n",
      "  Date: 2024-04-16\n",
      "  Cloud cover: 0.03637%\n",
      " ✓ (201.8 MB)ng B02 (as blue)...\n",
      " ✓ (204.8 MB)ng B03 (as green)...\n",
      " ✓ (215.5 MB)ng B04 (as red)...\n",
      " ✓ (221.9 MB)ng B08 (as nir)...\n",
      " ✓ (57.8 MB)ing B11 (as swir16)...\n",
      " ✓ (3.7 MB)ding SCL (as scl)...\n",
      "\n",
      "  ✓ Successfully downloaded 6 bands to: ./data/sentinel2/Chilika_Lake_2024-04-16\n",
      "\n",
      "============================================================\n",
      "Downloading: Chilika_Lake - post_monsoon\n",
      "============================================================\n",
      "\n",
      "  Scene: S2A_45QUB_20240928_0_L2A\n",
      "  Date: 2024-09-28\n",
      "  Cloud cover: 9.932241%\n",
      " ✓ (208.2 MB)ng B02 (as blue)...\n",
      " ✓ (207.1 MB)ng B03 (as green)...\n",
      " ✓ (205.5 MB)ng B04 (as red)...\n",
      " ✓ (204.3 MB)ng B08 (as nir)...\n",
      " ✓ (48.0 MB)ing B11 (as swir16)...\n",
      " ✓ (1.4 MB)ding SCL (as scl)...\n",
      "\n",
      "  ✓ Successfully downloaded 6 bands to: ./data/sentinel2/Chilika_Lake_2024-09-28\n",
      "\n",
      "============================================================\n",
      "SENTINEL-2 DOWNLOAD SUMMARY\n",
      "============================================================\n",
      "\n",
      "Hirakud_Reservoir:\n",
      "  pre_monsoon: ✓ Success\n",
      "    Path: ./data/sentinel2/Hirakud_Reservoir_2024-05-14\n",
      "    Scene: S2A_44QQJ_20240514_0_L2A\n",
      "    Cloud cover: 0.022878%\n",
      "    Bands: B02, B03, B04, B08, B11, SCL\n",
      "  post_monsoon: ✓ Success\n",
      "    Path: ./data/sentinel2/Hirakud_Reservoir_2024-10-06\n",
      "    Scene: S2B_44QQK_20241006_0_L2A\n",
      "    Cloud cover: 10.98659%\n",
      "    Bands: B02, B03, B04, B08, B11, SCL\n",
      "\n",
      "Chilika_Lake:\n",
      "  pre_monsoon: ✓ Success\n",
      "    Path: ./data/sentinel2/Chilika_Lake_2024-04-16\n",
      "    Scene: S2B_45QUC_20240416_0_L2A\n",
      "    Cloud cover: 0.03637%\n",
      "    Bands: B02, B03, B04, B08, B11, SCL\n",
      "  post_monsoon: ✓ Success\n",
      "    Path: ./data/sentinel2/Chilika_Lake_2024-09-28\n",
      "    Scene: S2A_45QUB_20240928_0_L2A\n",
      "    Cloud cover: 9.932241%\n",
      "    Bands: B02, B03, B04, B08, B11, SCL\n",
      "\n",
      "Total bands downloaded: 24\n",
      "\n",
      "✓ Download log saved to: ./data/sentinel2/download_log.json\n"
     ]
    }
   ],
   "source": [
    "# Download Sentinel-2 data\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING AVAILABLE SENTINEL-2 ASSETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First, let's see what assets are actually available\n",
    "for roi_name, periods in s2_items.items():\n",
    "    for period_name, item in periods.items():\n",
    "        print(f\"\\n{roi_name} - {period_name}:\")\n",
    "        print(f\"  Scene: {item.id}\")\n",
    "        print(f\"  Available assets: {list(item.assets.keys())[:10]}...\")  # Show first 10\n",
    "        break\n",
    "    break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOWNLOADING SENTINEL-2 BANDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "BAND_MAPPING = {\n",
    "    'B02': ['B02', 'blue', 'coastal'],\n",
    "    'B03': ['B03', 'green'],\n",
    "    'B04': ['B04', 'red'],\n",
    "    'B08': ['B08', 'nir', 'nir08'],\n",
    "    'B11': ['B11', 'swir16', 'swir1'],\n",
    "    'SCL': ['SCL', 'scl']\n",
    "}\n",
    "\n",
    "def find_band_asset(item, band_key):\n",
    "    \"\"\"Find the actual asset name for a band\"\"\"\n",
    "    possible_names = BAND_MAPPING.get(band_key, [band_key])\n",
    "    \n",
    "    for name in possible_names:\n",
    "        if name in item.assets:\n",
    "            return name\n",
    "        # Try lowercase\n",
    "        if name.lower() in item.assets:\n",
    "            return name.lower()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def download_sentinel2_bands_v2(item, roi_name, period_name, target_bands=['B02', 'B03', 'B04', 'B08', 'B11', 'SCL']):\n",
    "\n",
    "    scene_id = item.id\n",
    "    date = item.properties['datetime'][:10]\n",
    "    cloud_cover = item.properties.get('eo:cloud_cover', 'N/A')\n",
    "    \n",
    "    print(f\"\\n  Scene: {scene_id}\")\n",
    "    print(f\"  Date: {date}\")\n",
    "    print(f\"  Cloud cover: {cloud_cover}%\")\n",
    "    \n",
    "    # Create scene directory\n",
    "    scene_dir = f\"./data/sentinel2/{roi_name}_{date}\"\n",
    "    Path(scene_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    downloaded_files = {}\n",
    "    \n",
    "    for band_key in target_bands:\n",
    "        try:\n",
    "   \n",
    "            asset_name = find_band_asset(item, band_key)\n",
    "            \n",
    "            if asset_name:\n",
    "                asset = item.assets[asset_name]\n",
    "                url = asset.href\n",
    "                \n",
    "   \n",
    "                filename = f\"{scene_dir}/{band_key}.tif\"\n",
    "                \n",
    "                if not os.path.exists(filename):\n",
    "                    print(f\"    Downloading {band_key} (as {asset_name})...\", end='', flush=True)\n",
    "                    \n",
    "     \n",
    "                    response = requests.get(url, stream=True, timeout=300)\n",
    "                    response.raise_for_status()\n",
    "                    \n",
    "                    with open(filename, 'wb') as f:\n",
    "                        for chunk in response.iter_content(chunk_size=1024*1024):  # 1MB chunks\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "               \n",
    "                    if os.path.exists(filename) and os.path.getsize(filename) > 0:\n",
    "                        file_size_mb = os.path.getsize(filename) / (1024*1024)\n",
    "                        print(f\" ✓ ({file_size_mb:.1f} MB)\")\n",
    "                        downloaded_files[band_key] = filename\n",
    "                    else:\n",
    "                        print(f\" ✗ (file empty)\")\n",
    "                        \n",
    "                else:\n",
    "                    file_size_mb = os.path.getsize(filename) / (1024*1024)\n",
    "                    print(f\"    {band_key} already exists ✓ ({file_size_mb:.1f} MB)\")\n",
    "                    downloaded_files[band_key] = filename\n",
    "            else:\n",
    "                print(f\"    {band_key} not found in available assets\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Failed to download {band_key}: {str(e)[:80]}\")\n",
    "    \n",
    "    return scene_dir, downloaded_files\n",
    "\n",
    "downloaded_s2 = {}\n",
    "\n",
    "for roi_name, periods in s2_items.items():\n",
    "    downloaded_s2[roi_name] = {}\n",
    "    \n",
    "    for period_name, item in periods.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Downloading: {roi_name} - {period_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            scene_dir, files = download_sentinel2_bands_v2(\n",
    "                item,\n",
    "                roi_name,\n",
    "                period_name,\n",
    "                target_bands=['B02', 'B03', 'B04', 'B08', 'B11', 'SCL']\n",
    "            )\n",
    "            \n",
    "            downloaded_s2[roi_name][period_name] = {\n",
    "                'path': scene_dir,\n",
    "                'scene_id': item.id,\n",
    "                'date': item.properties['datetime'][:10],\n",
    "                'cloud_cover': item.properties.get('eo:cloud_cover', 'N/A'),\n",
    "                'files': files\n",
    "            }\n",
    "            \n",
    "            if len(files) > 0:\n",
    "                print(f\"\\n  ✓ Successfully downloaded {len(files)} bands to: {scene_dir}\")\n",
    "            else:\n",
    "                print(f\"\\n  ✗ No bands were downloaded\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ✗ Download failed: {e}\")\n",
    "            downloaded_s2[roi_name][period_name] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENTINEL-2 DOWNLOAD SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_bands = 0\n",
    "for roi_name, periods in downloaded_s2.items():\n",
    "    print(f\"\\n{roi_name}:\")\n",
    "    for period, data in periods.items():\n",
    "        if data and data['files']:\n",
    "            print(f\"  {period}: ✓ Success\")\n",
    "            print(f\"    Path: {data['path']}\")\n",
    "            print(f\"    Scene: {data['scene_id']}\")\n",
    "            print(f\"    Cloud cover: {data['cloud_cover']}%\")\n",
    "            print(f\"    Bands: {', '.join(data['files'].keys())}\")\n",
    "            total_bands += len(data['files'])\n",
    "        else:\n",
    "            print(f\"  {period}: ✗ Failed or no files downloaded\")\n",
    "\n",
    "print(f\"\\nTotal bands downloaded: {total_bands}\")\n",
    "\n",
    "\n",
    "with open('./data/sentinel2/download_log.json', 'w') as f:\n",
    "    log = {\n",
    "        'download_time': datetime.now().isoformat(),\n",
    "        'scenes': downloaded_s2\n",
    "    }\n",
    "    json.dump(log, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n✓ Download log saved to: ./data/sentinel2/download_log.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SENTINEL-1 PREPROCESSING - HANDLE VV AND VH\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing: Hirakud_Reservoir\n",
      "============================================================\n",
      "\n",
      "  Processing: Hirakud_Reservoir_pre_monsoon\n",
      "    Already extracted\n",
      "    Found VV: s1a-iw-grd-vv-20240523t002148-20240523t002213-053991-069045-001.tiff\n",
      "    Original shape: (16740, 25521)\n",
      "    Subset shape: (10044, 15312)\n",
      "    VV range: 9.54 to 39.99 dB\n",
      "    Applying Lee filter...\n",
      "    ✓ Saved: ./output/Hirakud_Reservoir/s1_pre_monsoon/S1_VV_processed.tif\n",
      "    Output shape: (10044, 15312)\n",
      "\n",
      "  Processing: Hirakud_Reservoir_post_monsoon\n",
      "    Already extracted\n",
      "    Found VV: s1a-iw-grd-vv-20241026t002147-20241026t002212-056266-06e3cf-001.tiff\n",
      "    Original shape: (16740, 25523)\n",
      "    Subset shape: (10044, 15314)\n",
      "    VV range: 10.00 to 39.91 dB\n",
      "    Applying Lee filter...\n",
      "    ✓ Saved: ./output/Hirakud_Reservoir/s1_post_monsoon/S1_VV_processed.tif\n",
      "    Output shape: (10044, 15314)\n",
      "\n",
      "============================================================\n",
      "Processing: Chilika_Lake\n",
      "============================================================\n",
      "\n",
      "  Processing: Chilika_Lake_pre_monsoon\n",
      "    Already extracted\n",
      "    Found VV: s1a-iw-grd-vv-20240530t001406-20240530t001431-054093-0693d5-001.tiff\n",
      "    Original shape: (16747, 25503)\n",
      "    Subset shape: (10048, 15302)\n",
      "    VV range: 9.54 to 40.19 dB\n",
      "    Applying Lee filter...\n",
      "    ✓ Saved: ./output/Chilika_Lake/s1_pre_monsoon/S1_VV_processed.tif\n",
      "    Output shape: (10048, 15302)\n",
      "\n",
      "  Processing: Chilika_Lake_post_monsoon\n",
      "    Already extracted\n",
      "    Found VV: s1a-iw-grd-vv-20241021t001405-20241021t001430-056193-06e0e9-001.tiff\n",
      "    Original shape: (16748, 25506)\n",
      "    Subset shape: (10049, 15303)\n",
      "    VV range: 8.45 to 41.59 dB\n",
      "    Applying Lee filter...\n",
      "    ✓ Saved: ./output/Chilika_Lake/s1_post_monsoon/S1_VV_processed.tif\n",
      "    Output shape: (10049, 15303)\n",
      "\n",
      "============================================================\n",
      "SENTINEL-1 PREPROCESSING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Hirakud_Reservoir:\n",
      "  pre_monsoon: ✓ VV - ./output/Hirakud_Reservoir/s1_pre_monsoon/S1_VV_processed.tif\n",
      "  post_monsoon: ✓ VV - ./output/Hirakud_Reservoir/s1_post_monsoon/S1_VV_processed.tif\n",
      "\n",
      "Chilika_Lake:\n",
      "  pre_monsoon: ✓ VV - ./output/Chilika_Lake/s1_pre_monsoon/S1_VV_processed.tif\n",
      "  post_monsoon: ✓ VV - ./output/Chilika_Lake/s1_post_monsoon/S1_VV_processed.tif\n",
      "\n",
      "Successfully processed: 4/4 scenes\n",
      "✓ Log saved\n"
     ]
    }
   ],
   "source": [
    "#Sentinel-1 Preprocessing\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SENTINEL-1 PREPROCESSING - HANDLE VV AND VH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import zipfile\n",
    "import glob\n",
    "\n",
    "def extract_sentinel1_zip(zip_path, extract_dir):\n",
    "    \"\"\"Extract Sentinel-1 zip file\"\"\"\n",
    "    print(f\"    Extracting: {os.path.basename(zip_path)}\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    return extract_dir\n",
    "\n",
    "def find_sentinel1_sar_band(scene_dir):\n",
    "    \"\"\"Find VV or VH band (prefer VV)\"\"\"\n",
    "    # Try VV first\n",
    "    vv_patterns = [\n",
    "        f\"{scene_dir}/**/*-vv-*.tiff\",\n",
    "        f\"{scene_dir}/**/*-vv-*.tif\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in vv_patterns:\n",
    "        files = glob.glob(pattern, recursive=True)\n",
    "        if files:\n",
    "            return files[0], 'VV'\n",
    "    \n",
    "    # Fall back to VH\n",
    "    vh_patterns = [\n",
    "        f\"{scene_dir}/**/*-vh-*.tiff\",\n",
    "        f\"{scene_dir}/**/*-vh-*.tif\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in vh_patterns:\n",
    "        files = glob.glob(pattern, recursive=True)\n",
    "        if files:\n",
    "            return files[0], 'VH'\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def lee_filter(img, window_size=5):\n",
    "    \"\"\"Apply Lee speckle filter\"\"\"\n",
    "    img = np.where(np.isfinite(img), img, -30)\n",
    "    mean = uniform_filter(img, size=window_size)\n",
    "    sqr_mean = uniform_filter(img**2, size=window_size)\n",
    "    variance = sqr_mean - mean**2\n",
    "    variance = np.where(variance < 0, 0, variance)\n",
    "    overall_variance = np.var(img[np.isfinite(img)])\n",
    "    weights = variance / (variance + overall_variance + 1e-10)\n",
    "    filtered = mean + weights * (img - mean)\n",
    "    return filtered\n",
    "\n",
    "def process_sentinel1_scene_flexible(scene_path, roi, output_dir):\n",
    "    \"\"\"\n",
    "    Process Sentinel-1 with flexible polarization\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Processing: {os.path.basename(scene_path)}\")\n",
    "    \n",
    "\n",
    "    zip_files = glob.glob(f\"{scene_path}/*.zip\")\n",
    "    if not zip_files:\n",
    "        print(f\"    ✗ No zip files found\")\n",
    "        return None\n",
    "    \n",
    "    zip_path = zip_files[0]\n",
    "    safe_dir = zip_path.replace('.zip', '.SAFE')\n",
    "    \n",
    "    if not os.path.exists(safe_dir):\n",
    "        print(f\"    Extracting...\")\n",
    "        extract_sentinel1_zip(zip_path, os.path.dirname(zip_path))\n",
    "    else:\n",
    "        print(f\"    Already extracted\")\n",
    "    \n",
    "\n",
    "    sar_path, polarization = find_sentinel1_sar_band(safe_dir)\n",
    "    \n",
    "    if not sar_path:\n",
    "        print(f\"    ✗ Could not find any SAR band\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"    Found {polarization}: {os.path.basename(sar_path)}\")\n",
    "    \n",
    "\n",
    "    with rasterio.open(sar_path) as src:\n",
    "        sar_linear = src.read(1)\n",
    "        original_shape = sar_linear.shape\n",
    "        print(f\"    Original shape: {original_shape}\")\n",
    "    \n",
    "\n",
    "    h, w = sar_linear.shape\n",
    "    \n",
    "\n",
    "    h_start = int(h * 0.2)\n",
    "    h_end = int(h * 0.8)\n",
    "    w_start = int(w * 0.2)\n",
    "    w_end = int(w * 0.8)\n",
    "    \n",
    "    sar_subset = sar_linear[h_start:h_end, w_start:w_end]\n",
    "    print(f\"    Subset shape: {sar_subset.shape}\")\n",
    "    \n",
    "\n",
    "    sar_subset = np.where(sar_subset > 0, sar_subset, 0.0001)\n",
    "    sar_db = 10 * np.log10(sar_subset)\n",
    "    \n",
    "    print(f\"    {polarization} range: {sar_db.min():.2f} to {sar_db.max():.2f} dB\")\n",
    "    \n",
    "\n",
    "    print(f\"    Applying Lee filter...\")\n",
    "    sar_filtered = lee_filter(sar_db, window_size=5)\n",
    "    \n",
    "\n",
    "    output_path = f\"{output_dir}/S1_{polarization}_processed.tif\"\n",
    "    \n",
    "\n",
    "    bbox = roi['bbox']\n",
    "    \n",
    "    from rasterio.transform import from_bounds\n",
    "    transform = from_bounds(bbox[0], bbox[1], bbox[2], bbox[3], \n",
    "                           sar_filtered.shape[1], sar_filtered.shape[0])\n",
    "    \n",
    "    profile = {\n",
    "        'driver': 'GTiff',\n",
    "        'dtype': rasterio.float32,\n",
    "        'width': sar_filtered.shape[1],\n",
    "        'height': sar_filtered.shape[0],\n",
    "        'count': 1,\n",
    "        'crs': 'EPSG:4326',\n",
    "        'transform': transform,\n",
    "        'compress': 'lzw'\n",
    "    }\n",
    "    \n",
    "    with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "        dst.write(sar_filtered.astype(rasterio.float32), 1)\n",
    "    \n",
    "    print(f\"    ✓ Saved: {output_path}\")\n",
    "    print(f\"    Output shape: {sar_filtered.shape}\")\n",
    "    \n",
    "    return output_path, polarization\n",
    "\n",
    "\n",
    "processed_s1 = {}\n",
    "\n",
    "for roi in rois:\n",
    "    roi_name = roi['name']\n",
    "    processed_s1[roi_name] = {}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {roi_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for period in ['pre_monsoon', 'post_monsoon']:\n",
    "        if roi_name in downloaded_s1 and period in downloaded_s1[roi_name]:\n",
    "            scene_data = downloaded_s1[roi_name][period]\n",
    "            scene_path = scene_data.get('path') if isinstance(scene_data, dict) else scene_data\n",
    "            \n",
    "            if scene_path:\n",
    "                output_dir = f\"./output/{roi_name}/s1_{period}\"\n",
    "                Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                try:\n",
    "                    result = process_sentinel1_scene_flexible(scene_path, roi, output_dir)\n",
    "                    if result:\n",
    "                        processed_path, pol = result\n",
    "                        processed_s1[roi_name][period] = {\n",
    "                            'path': processed_path,\n",
    "                            'polarization': pol\n",
    "                        }\n",
    "                    else:\n",
    "                        processed_s1[roi_name][period] = None\n",
    "                except Exception as e:\n",
    "                    print(f\"    ✗ Failed: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    processed_s1[roi_name][period] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENTINEL-1 PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "success_count = 0\n",
    "for roi_name, periods in processed_s1.items():\n",
    "    print(f\"\\n{roi_name}:\")\n",
    "    for period, data in periods.items():\n",
    "        if data:\n",
    "            if isinstance(data, dict):\n",
    "                print(f\"  {period}: ✓ {data['polarization']} - {data['path']}\")\n",
    "            else:\n",
    "                print(f\"  {period}: ✓ {data}\")\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"  {period}: ✗ Failed\")\n",
    "\n",
    "print(f\"\\nSuccessfully processed: {success_count}/4 scenes\")\n",
    "\n",
    "# Save log\n",
    "with open('./output/s1_preprocessing_log.json', 'w') as f:\n",
    "    json.dump(processed_s1, f, indent=2, default=str)\n",
    "print(\"✓ Log saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SENTINEL-2 PREPROCESSING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing: Hirakud_Reservoir\n",
      "============================================================\n",
      "\n",
      "  Processing: S2A_44QQJ_20240514_0_L2A\n",
      "  Reported cloud cover: 0.022878%\n",
      "    Calculating NDWI...\n",
      "    NDWI range: -1.000 to 1.000\n",
      "    NDWI shape: (10980, 10980)\n",
      "    Applying cloud mask...\n",
      "      Resampling SCL from (5490, 5490) to (10980, 10980)\n",
      "    Actual cloud coverage: 0.03%\n",
      "    ✓ Saved NDWI: ./output/Hirakud_Reservoir/s2_pre_monsoon/S2_NDWI.tif\n",
      "    Creating RGB composite...\n",
      "    ✓ Saved RGB: ./output/Hirakud_Reservoir/s2_pre_monsoon/S2_RGB.tif\n",
      "\n",
      "  Processing: S2B_44QQK_20241006_0_L2A\n",
      "  Reported cloud cover: 10.98659%\n",
      "    Calculating NDWI...\n",
      "    NDWI range: -1.000 to 1.000\n",
      "    NDWI shape: (10980, 10980)\n",
      "    Applying cloud mask...\n",
      "      Resampling SCL from (5490, 5490) to (10980, 10980)\n",
      "    Actual cloud coverage: 14.63%\n",
      "    ✓ Saved NDWI: ./output/Hirakud_Reservoir/s2_post_monsoon/S2_NDWI.tif\n",
      "    Creating RGB composite...\n",
      "    ✓ Saved RGB: ./output/Hirakud_Reservoir/s2_post_monsoon/S2_RGB.tif\n",
      "\n",
      "============================================================\n",
      "Processing: Chilika_Lake\n",
      "============================================================\n",
      "\n",
      "  Processing: S2B_45QUC_20240416_0_L2A\n",
      "  Reported cloud cover: 0.03637%\n",
      "    Calculating NDWI...\n",
      "    NDWI range: -1.000 to 1.000\n",
      "    NDWI shape: (10980, 10980)\n",
      "    Applying cloud mask...\n",
      "      Resampling SCL from (5490, 5490) to (10980, 10980)\n",
      "    Actual cloud coverage: 0.04%\n",
      "    ✓ Saved NDWI: ./output/Chilika_Lake/s2_pre_monsoon/S2_NDWI.tif\n",
      "    Creating RGB composite...\n",
      "    ✓ Saved RGB: ./output/Chilika_Lake/s2_pre_monsoon/S2_RGB.tif\n",
      "\n",
      "  Processing: S2A_45QUB_20240928_0_L2A\n",
      "  Reported cloud cover: 9.932241%\n",
      "    Calculating NDWI...\n",
      "    NDWI range: -1.000 to 1.000\n",
      "    NDWI shape: (10980, 10980)\n",
      "    Applying cloud mask...\n",
      "      Resampling SCL from (5490, 5490) to (10980, 10980)\n",
      "    Actual cloud coverage: 11.25%\n",
      "    ✓ Saved NDWI: ./output/Chilika_Lake/s2_post_monsoon/S2_NDWI.tif\n",
      "    Creating RGB composite...\n"
     ]
    }
   ],
   "source": [
    "#Preprocess Sentinel-2\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SENTINEL-2 PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def calculate_ndwi(green_path, nir_path):\n",
    "    \"\"\"\n",
    "    Calculate NDWI (Normalized Difference Water Index)\n",
    "    NDWI = (Green - NIR) / (Green + NIR)\n",
    "    \"\"\"\n",
    "    with rasterio.open(green_path) as green_src:\n",
    "        green = green_src.read(1).astype(float)\n",
    "        profile = green_src.profile.copy()\n",
    "    \n",
    "    with rasterio.open(nir_path) as nir_src:\n",
    "        nir = nir_src.read(1).astype(float)\n",
    "    \n",
    "\n",
    "    denominator = green + nir\n",
    "    ndwi = np.where(denominator != 0, (green - nir) / denominator, 0)\n",
    "    \n",
    "    return ndwi, profile\n",
    "\n",
    "def apply_cloud_mask(data, scl_path):\n",
    "    \"\"\"\n",
    "    Apply cloud mask using Scene Classification Layer (SCL)\n",
    "    Handles resolution mismatch by resampling SCL\n",
    "    \"\"\"\n",
    "    with rasterio.open(scl_path) as scl_src:\n",
    "        scl = scl_src.read(1)\n",
    "    \n",
    "\n",
    "    if scl.shape != data.shape:\n",
    "        print(f\"      Resampling SCL from {scl.shape} to {data.shape}\")\n",
    "        \n",
    "        zoom_factor_h = data.shape[0] / scl.shape[0]\n",
    "        zoom_factor_w = data.shape[1] / scl.shape[1]\n",
    "        \n",
    "        scl = zoom(scl, (zoom_factor_h, zoom_factor_w), order=0)\n",
    "    \n",
    "\n",
    "    cloud_mask = np.isin(scl, [3, 8, 9, 10])\n",
    "    \n",
    "\n",
    "    data_masked = data.copy()\n",
    "    data_masked[cloud_mask] = np.nan\n",
    "    \n",
    "    cloud_percentage = (np.sum(cloud_mask) / cloud_mask.size) * 100\n",
    "    \n",
    "    return data_masked, cloud_percentage\n",
    "\n",
    "def create_rgb_composite(scene_dir, output_path):\n",
    " \n",
    "    b04_path = f\"{scene_dir}/B04.tif\"  # Red\n",
    "    b03_path = f\"{scene_dir}/B03.tif\"  # Green\n",
    "    b02_path = f\"{scene_dir}/B02.tif\"  # Blue\n",
    "    \n",
    "    with rasterio.open(b04_path) as src:\n",
    "        red = src.read(1)\n",
    "        profile = src.profile.copy()\n",
    "    \n",
    "    with rasterio.open(b03_path) as src:\n",
    "        green = src.read(1)\n",
    "    \n",
    "    with rasterio.open(b02_path) as src:\n",
    "        blue = src.read(1)\n",
    "    \n",
    "    # Normalize to 0-1 for visualization\n",
    "    def normalize(band):\n",
    "        band = band.astype(float)\n",
    "        valid_pixels = band[band > 0]\n",
    "        if len(valid_pixels) > 0:\n",
    "            p2, p98 = np.percentile(valid_pixels, (2, 98))\n",
    "            return np.clip((band - p2) / (p98 - p2), 0, 1)\n",
    "        else:\n",
    "            return band\n",
    "    \n",
    "    red_norm = normalize(red)\n",
    "    green_norm = normalize(green)\n",
    "    blue_norm = normalize(blue)\n",
    "    \n",
    "    # Stack and save\n",
    "    rgb = np.stack([red_norm, green_norm, blue_norm])\n",
    "    \n",
    "    profile.update(count=3, dtype=rasterio.float32)\n",
    "    \n",
    "    with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "        dst.write(rgb.astype(rasterio.float32))\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def process_sentinel2_scene(scene_data, roi_name, period, output_dir):\n",
    " \n",
    "    scene_dir = scene_data['path']\n",
    "    scene_id = scene_data['scene_id']\n",
    "    cloud_cover = scene_data['cloud_cover']\n",
    "    \n",
    "    print(f\"\\n  Processing: {scene_id}\")\n",
    "    print(f\"  Reported cloud cover: {cloud_cover}%\")\n",
    "    \n",
    "    green_path = f\"{scene_dir}/B03.tif\"\n",
    "    nir_path = f\"{scene_dir}/B08.tif\"\n",
    "    scl_path = f\"{scene_dir}/SCL.tif\"\n",
    "    \n",
    "\n",
    "    if not os.path.exists(green_path):\n",
    "        print(f\"    ✗ Band files not found in {scene_dir}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "    print(f\"    Calculating NDWI...\")\n",
    "    ndwi, profile = calculate_ndwi(green_path, nir_path)\n",
    "    print(f\"    NDWI range: {np.nanmin(ndwi):.3f} to {np.nanmax(ndwi):.3f}\")\n",
    "    print(f\"    NDWI shape: {ndwi.shape}\")\n",
    "    \n",
    "\n",
    "    if os.path.exists(scl_path):\n",
    "        print(f\"    Applying cloud mask...\")\n",
    "        ndwi_masked, actual_cloud_pct = apply_cloud_mask(ndwi, scl_path)\n",
    "        print(f\"    Actual cloud coverage: {actual_cloud_pct:.2f}%\")\n",
    "    else:\n",
    "        print(f\"    Warning: No SCL file, skipping cloud masking\")\n",
    "        ndwi_masked = ndwi\n",
    "        actual_cloud_pct = 0\n",
    "    \n",
    "\n",
    "    ndwi_path = f\"{output_dir}/S2_NDWI.tif\"\n",
    "    profile.update(dtype=rasterio.float32, count=1, nodata=np.nan)\n",
    "    \n",
    "    with rasterio.open(ndwi_path, 'w', **profile) as dst:\n",
    "        dst.write(ndwi_masked.astype(rasterio.float32), 1)\n",
    "    \n",
    "    print(f\"    ✓ Saved NDWI: {ndwi_path}\")\n",
    "    \n",
    "\n",
    "    print(f\"    Creating RGB composite...\")\n",
    "    rgb_path = f\"{output_dir}/S2_RGB.tif\"\n",
    "    create_rgb_composite(scene_dir, rgb_path)\n",
    "    print(f\"    ✓ Saved RGB: {rgb_path}\")\n",
    "    \n",
    "    return {\n",
    "        'ndwi_path': ndwi_path,\n",
    "        'rgb_path': rgb_path,\n",
    "        'actual_cloud_pct': actual_cloud_pct,\n",
    "        'shape': ndwi_masked.shape\n",
    "    }\n",
    "\n",
    "\n",
    "processed_s2 = {}\n",
    "\n",
    "for roi_name, periods in downloaded_s2.items():\n",
    "    processed_s2[roi_name] = {}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {roi_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for period_name, scene_data in periods.items():\n",
    "        if scene_data and scene_data.get('path'):\n",
    "            output_dir = f\"./output/{roi_name}/s2_{period_name}\"\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            try:\n",
    "                result = process_sentinel2_scene(scene_data, roi_name, period_name, output_dir)\n",
    "                processed_s2[roi_name][period_name] = result\n",
    "            except Exception as e:\n",
    "                print(f\"    ✗ Processing failed: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                processed_s2[roi_name][period_name] = None\n",
    "        else:\n",
    "            print(f\"\\n  {period_name}: No data available\")\n",
    "            processed_s2[roi_name][period_name] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENTINEL-2 PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "success_count = 0\n",
    "for roi_name, periods in processed_s2.items():\n",
    "    print(f\"\\n{roi_name}:\")\n",
    "    for period, result in periods.items():\n",
    "        if result:\n",
    "            print(f\"  {period}: ✓\")\n",
    "            print(f\"    NDWI: {result['ndwi_path']}\")\n",
    "            print(f\"    RGB: {result['rgb_path']}\")\n",
    "            print(f\"    Cloud: {result['actual_cloud_pct']:.1f}%\")\n",
    "            print(f\"    Shape: {result['shape']}\")\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"  {period}: ✗ Failed\")\n",
    "\n",
    "print(f\"\\nSuccessfully processed: {success_count}/4 scenes\")\n",
    "\n",
    "\n",
    "with open('./output/s2_preprocessing_log.json', 'w') as f:\n",
    "    json.dump(processed_s2, f, indent=2, default=str)\n",
    "print(\"✓ Log saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COREGISTRATION: ALIGN S1 AND S2\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Coregistering: Hirakud_Reservoir\n",
      "============================================================\n",
      "\n",
      "  Period: pre_monsoon\n",
      "    Reference (S2): S2_NDWI.tif\n",
      "    Source (S1): S1_VV_processed.tif\n",
      "    Coregistering S1 to S2 grid...\n",
      "    ✓ Coregistration successful\n",
      "      Shape: (10980, 10980)\n",
      "\n",
      "  Period: post_monsoon\n",
      "    Reference (S2): S2_NDWI.tif\n",
      "    Source (S1): S1_VV_processed.tif\n",
      "    Coregistering S1 to S2 grid...\n",
      "    ✓ Coregistration successful\n",
      "      Shape: (10980, 10980)\n",
      "\n",
      "============================================================\n",
      "Coregistering: Chilika_Lake\n",
      "============================================================\n",
      "\n",
      "  Period: pre_monsoon\n",
      "    Reference (S2): S2_NDWI.tif\n",
      "    Source (S1): S1_VV_processed.tif\n",
      "    Coregistering S1 to S2 grid...\n",
      "    ✓ Coregistration successful\n",
      "      Shape: (10980, 10980)\n",
      "\n",
      "  Period: post_monsoon\n",
      "    Reference (S2): S2_NDWI.tif\n",
      "    Source (S1): S1_VV_processed.tif\n",
      "    Coregistering S1 to S2 grid...\n",
      "    ✓ Coregistration successful\n",
      "      Shape: (10980, 10980)\n",
      "\n",
      "============================================================\n",
      "COREGISTRATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Hirakud_Reservoir:\n",
      "  pre_monsoon: ✓ Shape (10980, 10980)\n",
      "  post_monsoon: ✓ Shape (10980, 10980)\n",
      "\n",
      "Chilika_Lake:\n",
      "  pre_monsoon: ✓ Shape (10980, 10980)\n",
      "  post_monsoon: ✓ Shape (10980, 10980)\n",
      "\n",
      "Successfully coregistered: 4/4 scene pairs\n",
      "✓ Log saved\n"
     ]
    }
   ],
   "source": [
    "#Coregister\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COREGISTRATION: ALIGN S1 AND S2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "def coregister_to_reference(src_path, ref_path, output_path):\n",
    "   \n",
    "    with rasterio.open(ref_path) as ref:\n",
    "        ref_crs = ref.crs\n",
    "        ref_transform = ref.transform\n",
    "        ref_width = ref.width\n",
    "        ref_height = ref.height\n",
    "    \n",
    " \n",
    "    with rasterio.open(src_path) as src:\n",
    "\n",
    "        src_data = src.read(1)\n",
    "        \n",
    " \n",
    "        dst_data = np.empty((ref_height, ref_width), dtype=src_data.dtype)\n",
    "        \n",
    "\n",
    "        reproject(\n",
    "            source=src_data,\n",
    "            destination=dst_data,\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src.crs,\n",
    "            dst_transform=ref_transform,\n",
    "            dst_crs=ref_crs,\n",
    "            resampling=Resampling.bilinear\n",
    "        )\n",
    "        \n",
    "\n",
    "        profile = src.profile.copy()\n",
    "        profile.update({\n",
    "            'crs': ref_crs,\n",
    "            'transform': ref_transform,\n",
    "            'width': ref_width,\n",
    "            'height': ref_height\n",
    "        })\n",
    "        \n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(dst_data, 1)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "coregistered = {}\n",
    "\n",
    "\n",
    "roi_names = ['Hirakud_Reservoir', 'Chilika_Lake']\n",
    "\n",
    "for roi_name in roi_names:\n",
    "    coregistered[roi_name] = {}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Coregistering: {roi_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for period in ['pre_monsoon', 'post_monsoon']:\n",
    "        print(f\"\\n  Period: {period}\")\n",
    "        \n",
    "\n",
    "        s1_data = processed_s1.get(roi_name, {}).get(period)\n",
    "        s2_data = processed_s2.get(roi_name, {}).get(period)\n",
    "        \n",
    "        if not s1_data or not s2_data:\n",
    "            print(f\"    ✗ Missing data for {period}\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "        if isinstance(s1_data, dict):\n",
    "            s1_path = s1_data['path']\n",
    "        else:\n",
    "            s1_path = s1_data\n",
    "        \n",
    " \n",
    "        s2_ndwi_path = s2_data['ndwi_path']\n",
    "        \n",
    "        print(f\"    Reference (S2): {os.path.basename(s2_ndwi_path)}\")\n",
    "        print(f\"    Source (S1): {os.path.basename(s1_path)}\")\n",
    "        \n",
    "\n",
    "        output_dir = f\"./output/{roi_name}/coregistered_{period}\"\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "\n",
    "        s1_coreg_path = f\"{output_dir}/S1_coregistered.tif\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"    Coregistering S1 to S2 grid...\")\n",
    "            coregister_to_reference(s1_path, s2_ndwi_path, s1_coreg_path)\n",
    "            \n",
    " \n",
    "            with rasterio.open(s1_coreg_path) as s1_src:\n",
    "                s1_shape = (s1_src.height, s1_src.width)\n",
    "            \n",
    "            with rasterio.open(s2_ndwi_path) as s2_src:\n",
    "                s2_shape = (s2_src.height, s2_src.width)\n",
    "            \n",
    "            if s1_shape == s2_shape:\n",
    "                print(f\"    ✓ Coregistration successful\")\n",
    "                print(f\"      Shape: {s1_shape}\")\n",
    "                \n",
    "                coregistered[roi_name][period] = {\n",
    "                    's1_coreg': s1_coreg_path,\n",
    "                    's2_ndwi': s2_ndwi_path,\n",
    "                    's2_rgb': s2_data['rgb_path'],\n",
    "                    'shape': s1_shape\n",
    "                }\n",
    "            else:\n",
    "                print(f\"    ✗ Shape mismatch!\")\n",
    "                print(f\"      S1: {s1_shape}, S2: {s2_shape}\")\n",
    "                coregistered[roi_name][period] = None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Coregistration failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            coregistered[roi_name][period] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COREGISTRATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "success_count = 0\n",
    "for roi_name, periods in coregistered.items():\n",
    "    print(f\"\\n{roi_name}:\")\n",
    "    for period, data in periods.items():\n",
    "        if data:\n",
    "            print(f\"  {period}: ✓ Shape {data['shape']}\")\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"  {period}: ✗ Failed\")\n",
    "\n",
    "print(f\"\\nSuccessfully coregistered: {success_count}/4 scene pairs\")\n",
    "\n",
    "# Save log\n",
    "with open('./output/coregistration_log.json', 'w') as f:\n",
    "    json.dump(coregistered, f, indent=2, default=str)\n",
    "print(\"✓ Log saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHANGE DETECTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Change Detection: Hirakud_Reservoir\n",
      "============================================================\n",
      "\n",
      "  Analyzing S2 change:\n",
      "    Water gain: 4.68% (5,645,948 pixels)\n",
      "    Water loss: 1.81% (2,178,565 pixels)\n",
      "    Net change: +2.88%\n",
      "    Persistent water: 44,235 pixels\n",
      "    ✓ Saved: ./output/Hirakud_Reservoir/change_detection/s2_change_map.tif\n",
      "\n",
      "  Analyzing COMBINED change:\n",
      "    Water gain: 4.66% (5,623,483 pixels)\n",
      "    Water loss: 3.40% (4,102,789 pixels)\n",
      "    Net change: +1.26%\n",
      "    Persistent water: 78,056 pixels\n",
      "    ✓ Saved: ./output/Hirakud_Reservoir/change_detection/combined_change_map.tif\n",
      "\n",
      "============================================================\n",
      "Change Detection: Chilika_Lake\n",
      "============================================================\n",
      "\n",
      "  Analyzing S2 change:\n",
      "    Water gain: 72.69% (87,639,160 pixels)\n",
      "    Water loss: 0.38% (456,485 pixels)\n",
      "    Net change: +72.31%\n",
      "    Persistent water: 2,482,314 pixels\n",
      "    ✓ Saved: ./output/Chilika_Lake/change_detection/s2_change_map.tif\n",
      "\n",
      "  Analyzing COMBINED change:\n",
      "    Water gain: 74.10% (89,340,787 pixels)\n",
      "    Water loss: 0.37% (445,077 pixels)\n",
      "    Net change: +73.74%\n",
      "    Persistent water: 2,499,752 pixels\n",
      "    ✓ Saved: ./output/Chilika_Lake/change_detection/combined_change_map.tif\n",
      "\n",
      "============================================================\n",
      "CHANGE DETECTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Hirakud_Reservoir:\n",
      "  S2:\n",
      "    Net water change: +2.88%\n",
      "    Water gain: 4.68%\n",
      "    Water loss: 1.81%\n",
      "  COMBINED:\n",
      "    Net water change: +1.26%\n",
      "    Water gain: 4.66%\n",
      "    Water loss: 3.40%\n",
      "\n",
      "Chilika_Lake:\n",
      "  S2:\n",
      "    Net water change: +72.31%\n",
      "    Water gain: 72.69%\n",
      "    Water loss: 0.38%\n",
      "  COMBINED:\n",
      "    Net water change: +73.74%\n",
      "    Water gain: 74.10%\n",
      "    Water loss: 0.37%\n",
      "\n",
      "✓ Statistics saved to: ./output/change_detection_stats.json\n",
      "✓ Change detection complete!\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 13: Change Detection Analysis\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHANGE DETECTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if prerequisites exist\n",
    "if 'water_masks' not in globals():\n",
    "    print(\"\\n⚠️  ERROR: water_masks not found!\")\n",
    "    print(\"You need to run CHUNK 12 first to generate water masks.\")\n",
    "    print(\"\\nAttempting to load from saved files...\")\n",
    "    \n",
    "    # Try to reconstruct from saved files\n",
    "    if Path('./output/water_masks_log.json').exists():\n",
    "        print(\"✓ Found water_masks_log.json, loading...\")\n",
    "        with open('./output/water_masks_log.json', 'r') as f:\n",
    "            water_masks_meta = json.load(f)\n",
    "        \n",
    "        # Reconstruct water_masks with actual arrays\n",
    "        water_masks = {}\n",
    "        for roi_name in ['Hirakud_Reservoir', 'Chilika_Lake']:\n",
    "            water_masks[roi_name] = {}\n",
    "            for period in ['pre_monsoon', 'post_monsoon']:\n",
    "                if roi_name in water_masks_meta and period in water_masks_meta[roi_name]:\n",
    "                    paths = water_masks_meta[roi_name][period]\n",
    "                    \n",
    "                    # Load the masks from files\n",
    "                    with rasterio.open(paths['s2_water']) as src:\n",
    "                        s2_mask = src.read(1)\n",
    "                    \n",
    "                    with rasterio.open(paths['combined_water']) as src:\n",
    "                        combined_mask = src.read(1)\n",
    "                    \n",
    "                    water_masks[roi_name][period] = {\n",
    "                        's2_mask': s2_mask,\n",
    "                        'combined_mask': combined_mask,\n",
    "                        's2_water': paths['s2_water'],\n",
    "                        'combined_water': paths['combined_water']\n",
    "                    }\n",
    "        print(\"✓ Water masks reconstructed from saved files\")\n",
    "    else:\n",
    "        print(\"\\n❌ Cannot proceed without water masks.\")\n",
    "        print(\"Please run CHUNK 12 first, then run this cell again.\")\n",
    "        raise SystemExit(\"Missing prerequisites\")\n",
    "\n",
    "def compute_change(pre_mask, post_mask):\n",
    "    \"\"\"\n",
    "    Compute change between pre and post monsoon\n",
    "    Returns:\n",
    "    - water_gain: areas that became water\n",
    "    - water_loss: areas that lost water\n",
    "    - no_change_water: persistent water\n",
    "    - no_change_land: persistent land\n",
    "    \"\"\"\n",
    "    water_gain = (pre_mask == 0) & (post_mask == 1)\n",
    "    water_loss = (pre_mask == 1) & (post_mask == 0)\n",
    "    no_change_water = (pre_mask == 1) & (post_mask == 1)\n",
    "    no_change_land = (pre_mask == 0) & (post_mask == 0)\n",
    "    \n",
    "    # Create change map: 0=no change land, 1=persistent water, 2=water gain, 3=water loss\n",
    "    change_map = np.zeros_like(pre_mask, dtype=np.uint8)\n",
    "    change_map[no_change_water] = 1\n",
    "    change_map[water_gain] = 2\n",
    "    change_map[water_loss] = 3\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_pixels = pre_mask.size\n",
    "    stats = {\n",
    "        'water_gain_pixels': int(np.sum(water_gain)),\n",
    "        'water_loss_pixels': int(np.sum(water_loss)),\n",
    "        'persistent_water_pixels': int(np.sum(no_change_water)),\n",
    "        'persistent_land_pixels': int(np.sum(no_change_land)),\n",
    "        'water_gain_pct': (np.sum(water_gain) / total_pixels) * 100,\n",
    "        'water_loss_pct': (np.sum(water_loss) / total_pixels) * 100,\n",
    "        'net_change_pct': ((np.sum(water_gain) - np.sum(water_loss)) / total_pixels) * 100\n",
    "    }\n",
    "    \n",
    "    return change_map, stats\n",
    "\n",
    "# Perform change detection for each ROI\n",
    "change_results = {}\n",
    "\n",
    "for roi_name in water_masks.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Change Detection: {roi_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get pre and post masks\n",
    "    pre_data = water_masks[roi_name].get('pre_monsoon')\n",
    "    post_data = water_masks[roi_name].get('post_monsoon')\n",
    "    \n",
    "    if not pre_data or not post_data:\n",
    "        print(f\"  ✗ Missing data\")\n",
    "        continue\n",
    "    \n",
    "    output_dir = f\"./output/{roi_name}/change_detection\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Analyze each sensor combination\n",
    "    change_results[roi_name] = {}\n",
    "    \n",
    "    for sensor_type in ['s2', 'combined']:\n",
    "        mask_key = f'{sensor_type}_mask'\n",
    "        print(f\"\\n  Analyzing {sensor_type.upper()} change:\")\n",
    "        \n",
    "        pre_mask = pre_data[mask_key]\n",
    "        post_mask = post_data[mask_key]\n",
    "        \n",
    "        # Compute change\n",
    "        change_map, stats = compute_change(pre_mask, post_mask)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"    Water gain: {stats['water_gain_pct']:.2f}% ({stats['water_gain_pixels']:,} pixels)\")\n",
    "        print(f\"    Water loss: {stats['water_loss_pct']:.2f}% ({stats['water_loss_pixels']:,} pixels)\")\n",
    "        print(f\"    Net change: {stats['net_change_pct']:+.2f}%\")\n",
    "        print(f\"    Persistent water: {stats['persistent_water_pixels']:,} pixels\")\n",
    "        \n",
    "        # Save change map\n",
    "        change_path = f\"{output_dir}/{sensor_type}_change_map.tif\"\n",
    "        \n",
    "        # Get profile from one of the water masks\n",
    "        with rasterio.open(pre_data[f'{sensor_type}_water']) as src:\n",
    "            profile = src.profile.copy()\n",
    "        \n",
    "        profile.update(dtype=rasterio.uint8, count=1, nodata=255)\n",
    "        \n",
    "        with rasterio.open(change_path, 'w', **profile) as dst:\n",
    "            dst.write(change_map, 1)\n",
    "        \n",
    "        print(f\"    ✓ Saved: {change_path}\")\n",
    "        \n",
    "        change_results[roi_name][sensor_type] = {\n",
    "            'change_map_path': change_path,\n",
    "            'statistics': stats,\n",
    "            'change_map_array': change_map\n",
    "        }\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHANGE DETECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for roi_name, sensors in change_results.items():\n",
    "    print(f\"\\n{roi_name}:\")\n",
    "    for sensor_type, data in sensors.items():\n",
    "        stats = data['statistics']\n",
    "        print(f\"  {sensor_type.upper()}:\")\n",
    "        print(f\"    Net water change: {stats['net_change_pct']:+.2f}%\")\n",
    "        print(f\"    Water gain: {stats['water_gain_pct']:.2f}%\")\n",
    "        print(f\"    Water loss: {stats['water_loss_pct']:.2f}%\")\n",
    "\n",
    "# Save statistics to JSON\n",
    "stats_log = {}\n",
    "for roi_name, sensors in change_results.items():\n",
    "    stats_log[roi_name] = {}\n",
    "    for sensor_type, data in sensors.items():\n",
    "        stats_log[roi_name][sensor_type] = {\n",
    "            'change_map': data['change_map_path'],\n",
    "            'statistics': data['statistics']\n",
    "        }\n",
    "\n",
    "with open('./output/change_detection_stats.json', 'w') as f:\n",
    "    json.dump(stats_log, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Statistics saved to: ./output/change_detection_stats.json\")\n",
    "print(\"✓ Change detection complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING WATER MASKS (S2 & COMBINED)\n",
      "============================================================\n",
      "\n",
      "⚠️  ERROR: coregistered not found!\n",
      "You need to run the coregistration chunk first.\n",
      "\n",
      "Attempting to load from saved files...\n",
      "✓ Found coregistration_log.json, loading...\n",
      "✓ Coregistered data loaded from saved files\n",
      "\n",
      "============================================================\n",
      "Processing ROI: Hirakud_Reservoir\n",
      "============================================================\n",
      "\n",
      "  Period: pre_monsoon\n",
      "    ✓ S2 water mask saved: ./output/Hirakud_Reservoir/water_masks_pre_monsoon/s2_water_mask.tif\n",
      "    ✓ Combined water mask saved: ./output/Hirakud_Reservoir/water_masks_pre_monsoon/combined_water_mask.tif\n",
      "\n",
      "  Period: post_monsoon\n",
      "    ✓ S2 water mask saved: ./output/Hirakud_Reservoir/water_masks_post_monsoon/s2_water_mask.tif\n",
      "    ✓ Combined water mask saved: ./output/Hirakud_Reservoir/water_masks_post_monsoon/combined_water_mask.tif\n",
      "\n",
      "============================================================\n",
      "Processing ROI: Chilika_Lake\n",
      "============================================================\n",
      "\n",
      "  Period: pre_monsoon\n",
      "    ✓ S2 water mask saved: ./output/Chilika_Lake/water_masks_pre_monsoon/s2_water_mask.tif\n",
      "    ✓ Combined water mask saved: ./output/Chilika_Lake/water_masks_pre_monsoon/combined_water_mask.tif\n",
      "\n",
      "  Period: post_monsoon\n",
      "    ✓ S2 water mask saved: ./output/Chilika_Lake/water_masks_post_monsoon/s2_water_mask.tif\n",
      "    ✓ Combined water mask saved: ./output/Chilika_Lake/water_masks_post_monsoon/combined_water_mask.tif\n",
      "\n",
      "============================================================\n",
      "WATER MASK GENERATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Hirakud_Reservoir:\n",
      "  pre_monsoon: ✓ Masks generated\n",
      "  post_monsoon: ✓ Masks generated\n",
      "\n",
      "Chilika_Lake:\n",
      "  pre_monsoon: ✓ Masks generated\n",
      "  post_monsoon: ✓ Masks generated\n",
      "\n",
      "✓ Water masks saved and logged successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate Water Masks\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING WATER MASKS (S2 & COMBINED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "if 'coregistered' not in globals():\n",
    "    print(\"\\n⚠️  ERROR: coregistered not found!\")\n",
    "    print(\"You need to run the coregistration chunk first.\")\n",
    "    print(\"\\nAttempting to load from saved files...\")\n",
    "    \n",
    "\n",
    "    if Path('./output/coregistration_log.json').exists():\n",
    "        print(\"✓ Found coregistration_log.json, loading...\")\n",
    "        with open('./output/coregistration_log.json', 'r') as f:\n",
    "            coregistered = json.load(f)\n",
    "        print(\"✓ Coregistered data loaded from saved files\")\n",
    "    else:\n",
    "        print(\"\\n❌ Cannot proceed without coregistered data.\")\n",
    "        print(\"Please run the coregistration chunk first.\")\n",
    "        raise SystemExit(\"Missing prerequisites\")\n",
    "\n",
    "water_masks = {}\n",
    "\n",
    "\n",
    "NDWI_THRESHOLD = 0.0    # NDWI > 0 = water\n",
    "S1_THRESHOLD_PERCENTILE = 90  # Top 10% backscatter = likely water (adjustable)\n",
    "\n",
    "for roi_name, periods in coregistered.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing ROI: {roi_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "   \n",
    "    water_masks[roi_name] = {}\n",
    "   \n",
    "    for period, data in periods.items():\n",
    "        if not data:\n",
    "            print(f\"  ✗ Missing data for {period}\")\n",
    "            continue\n",
    "       \n",
    "        print(f\"\\n  Period: {period}\")\n",
    "       \n",
    "        s2_ndwi_path = data['s2_ndwi']\n",
    "        s1_coreg_path = data['s1_coreg']\n",
    "        output_dir = f\"./output/{roi_name}/water_masks_{period}\"\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "       \n",
    "        \n",
    "        with rasterio.open(s2_ndwi_path) as src:\n",
    "            ndwi = src.read(1)\n",
    "            profile = src.profile.copy()\n",
    "       \n",
    "        \n",
    "        with rasterio.open(s1_coreg_path) as src:\n",
    "            s1_data = src.read(1)\n",
    "       \n",
    "        \n",
    "        s2_mask = (ndwi > NDWI_THRESHOLD).astype(np.uint8)\n",
    "       \n",
    "        s2_mask_path = f\"{output_dir}/s2_water_mask.tif\"\n",
    "        profile.update(dtype=rasterio.uint8, count=1, nodata=255)\n",
    "       \n",
    "        with rasterio.open(s2_mask_path, 'w', **profile) as dst:\n",
    "            dst.write(s2_mask, 1)\n",
    "       \n",
    "        print(f\"    ✓ S2 water mask saved: {s2_mask_path}\")\n",
    "       \n",
    "        \n",
    "        s1_valid = s1_data[np.isfinite(s1_data)]\n",
    "        if len(s1_valid) > 0:\n",
    "            threshold_s1 = np.percentile(s1_valid, S1_THRESHOLD_PERCENTILE)\n",
    "        else:\n",
    "            threshold_s1 = 0\n",
    "       \n",
    "        s1_mask = (s1_data > threshold_s1).astype(np.uint8)\n",
    "        combined_mask = ((s2_mask == 1) | (s1_mask == 1)).astype(np.uint8)\n",
    "       \n",
    "        combined_mask_path = f\"{output_dir}/combined_water_mask.tif\"\n",
    "       \n",
    "        with rasterio.open(combined_mask_path, 'w', **profile) as dst:\n",
    "            dst.write(combined_mask, 1)\n",
    "       \n",
    "        print(f\"    ✓ Combined water mask saved: {combined_mask_path}\")\n",
    "       \n",
    "        \n",
    "        water_masks[roi_name][period] = {\n",
    "            's2_mask': s2_mask,\n",
    "            'combined_mask': combined_mask,\n",
    "            's2_water': s2_mask_path,\n",
    "            'combined_water': combined_mask_path\n",
    "        }\n",
    "\n",
    "\n",
    "water_masks_log = {}\n",
    "for roi_name, periods in water_masks.items():\n",
    "    water_masks_log[roi_name] = {}\n",
    "    for period, data in periods.items():\n",
    "        water_masks_log[roi_name][period] = {\n",
    "            's2_water': data['s2_water'],\n",
    "            'combined_water': data['combined_water']\n",
    "        }\n",
    "\n",
    "with open('./output/water_masks_log.json', 'w') as f:\n",
    "    json.dump(water_masks_log, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WATER MASK GENERATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "for roi_name, periods in water_masks.items():\n",
    "    print(f\"\\n{roi_name}:\")\n",
    "    for period, data in periods.items():\n",
    "        print(f\"  {period}: ✓ Masks generated\")\n",
    "        \n",
    "print(\"\\n✓ Water masks saved and logged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING VISUALIZATIONS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Creating visualizations: Hirakud_Reservoir\n",
      "============================================================\n",
      "\n",
      "  Creating change detection visualization...\n",
      "  ✓ Saved: ./output/Hirakud_Reservoir/visualizations/Hirakud_Reservoir_change_visualization.png\n",
      "\n",
      "  Creating water mask comparison...\n",
      "  ✓ Saved: ./output/Hirakud_Reservoir/visualizations/Hirakud_Reservoir_water_comparison.png\n",
      "\n",
      "============================================================\n",
      "Creating visualizations: Chilika_Lake\n",
      "============================================================\n",
      "\n",
      "  Creating change detection visualization...\n",
      "  ✓ Saved: ./output/Chilika_Lake/visualizations/Chilika_Lake_change_visualization.png\n",
      "\n",
      "  Creating water mask comparison...\n",
      "  ✓ Saved: ./output/Chilika_Lake/visualizations/Chilika_Lake_water_comparison.png\n",
      "\n",
      "============================================================\n",
      "VISUALIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Hirakud_Reservoir:\n",
      "  Change visualization: ./output/Hirakud_Reservoir/visualizations/Hirakud_Reservoir_change_visualization.png\n",
      "  Water comparison: ./output/Hirakud_Reservoir/visualizations/Hirakud_Reservoir_water_comparison.png\n",
      "\n",
      "Chilika_Lake:\n",
      "  Change visualization: ./output/Chilika_Lake/visualizations/Chilika_Lake_change_visualization.png\n",
      "  Water comparison: ./output/Chilika_Lake/visualizations/Chilika_Lake_water_comparison.png\n",
      "\n",
      "✓ All visualizations saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create Visualization Maps\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def create_change_visualization(roi_name, change_data, pre_rgb_path, post_rgb_path, output_dir):\n",
    "    \n",
    "    with rasterio.open(pre_rgb_path) as src:\n",
    "        pre_rgb = src.read([1, 2, 3]).transpose(1, 2, 0)\n",
    "    \n",
    "    with rasterio.open(post_rgb_path) as src:\n",
    "        post_rgb = src.read([1, 2, 3]).transpose(1, 2, 0)\n",
    "    \n",
    "    change_map = change_data['change_map_array']\n",
    "    stats = change_data['statistics']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    fig.suptitle(f'Water Extent Change Detection - {roi_name.replace(\"_\", \" \")}', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.imshow(pre_rgb)\n",
    "    ax1.set_title('Pre-Monsoon (Sentinel-2 RGB)', fontsize=12, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.imshow(post_rgb)\n",
    "    ax2.set_title('Post-Monsoon (Sentinel-2 RGB)', fontsize=12, fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    colors = ['white', 'blue', 'cyan', 'red']\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    im = ax3.imshow(change_map, cmap=cmap, vmin=0, vmax=3)\n",
    "    ax3.set_title('Water Extent Change Map', fontsize=12, fontweight='bold')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    legend_elements = [\n",
    "        mpatches.Patch(color='white', label='No Change (Land)'),\n",
    "        mpatches.Patch(color='blue', label='Persistent Water'),\n",
    "        mpatches.Patch(color='cyan', label='Water Gain'),\n",
    "        mpatches.Patch(color='red', label='Water Loss')\n",
    "    ]\n",
    "    ax3.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    stats_text = f\"\"\"\n",
    "    CHANGE DETECTION STATISTICS\n",
    "    {'='*40}\n",
    "    \n",
    "    Water Gain:\n",
    "      • Pixels: {stats['water_gain_pixels']:,}\n",
    "      • Percentage: {stats['water_gain_pct']:.2f}%\n",
    "    \n",
    "    Water Loss:\n",
    "      • Pixels: {stats['water_loss_pixels']:,}\n",
    "      • Percentage: {stats['water_loss_pct']:.2f}%\n",
    "    \n",
    "    Net Change: {stats['net_change_pct']:+.2f}%\n",
    "    \n",
    "    Persistent Water:\n",
    "      • Pixels: {stats['persistent_water_pixels']:,}\n",
    "    \n",
    "    Persistent Land:\n",
    "      • Pixels: {stats['persistent_land_pixels']:,}\n",
    "    \n",
    "    {'='*40}\n",
    "    Total analyzed pixels: {stats['persistent_water_pixels'] + stats['persistent_land_pixels'] + stats['water_gain_pixels'] + stats['water_loss_pixels']:,}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax4.text(0.1, 0.5, stats_text, fontsize=11, family='monospace',\n",
    "             verticalalignment='center', transform=ax4.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = f\"{output_dir}/{roi_name}_change_visualization.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  ✓ Saved: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def create_side_by_side_comparison(roi_name, pre_mask, post_mask, output_dir):\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f'Water Mask Comparison - {roi_name.replace(\"_\", \" \")}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    ax1.imshow(pre_mask, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax1.set_title('Pre-Monsoon Water Extent', fontsize=12)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    ax2.imshow(post_mask, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax2.set_title('Post-Monsoon Water Extent', fontsize=12)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = f\"{output_dir}/{roi_name}_water_comparison.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  ✓ Saved: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "visualization_outputs = {}\n",
    "\n",
    "for roi_name in change_results.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Creating visualizations: {roi_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    output_dir = f\"./output/{roi_name}/visualizations\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pre_data = water_masks[roi_name]['pre_monsoon']\n",
    "    post_data = water_masks[roi_name]['post_monsoon']\n",
    "    change_data = change_results[roi_name]['s2']\n",
    "    \n",
    "    pre_rgb = coregistered[roi_name]['pre_monsoon']['s2_rgb']\n",
    "    post_rgb = coregistered[roi_name]['post_monsoon']['s2_rgb']\n",
    "    \n",
    "    print(\"\\n  Creating change detection visualization...\")\n",
    "    vis_path = create_change_visualization(\n",
    "        roi_name, \n",
    "        change_data,\n",
    "        pre_rgb,\n",
    "        post_rgb,\n",
    "        output_dir\n",
    "    )\n",
    "    \n",
    "    print(\"\\n  Creating water mask comparison...\")\n",
    "    comp_path = create_side_by_side_comparison(\n",
    "        roi_name,\n",
    "        pre_data['s2_mask'],\n",
    "        post_data['s2_mask'],\n",
    "        output_dir\n",
    "    )\n",
    "    \n",
    "    visualization_outputs[roi_name] = {\n",
    "        'change_visualization': vis_path,\n",
    "        'water_comparison': comp_path\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for roi_name, paths in visualization_outputs.items():\n",
    "    print(f\"\\n{roi_name}:\")\n",
    "    print(f\"  Change visualization: {paths['change_visualization']}\")\n",
    "    print(f\"  Water comparison: {paths['water_comparison']}\")\n",
    "\n",
    "print(\"\\n✓ All visualizations saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
